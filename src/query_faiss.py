"""
query_faiss.py

This script implements a job search pipeline using a FAISS vector database 
and Google Gemini for refining search results. It performs the following tasks:

1. Loads a prebuilt FAISS index (`job_index.faiss`) for efficient job retrieval.
2. Loads job listings from `job_data.json`.
3. Uses a Hugging Face model (`sentence-transformers/all-MiniLM-L6-v2`) 
   to generate embeddings for queries.
4. Searches the FAISS index for the most relevant job listings.
5. Sends the retrieved job listings to Google Gemini for refinement.
6. Logs user queries, retrieved job results, and AI-generated responses 
   into `query_logs.json`.

Usage:
- Call `search_jobs(query, k=10)` with a job-related query to retrieve 
  relevant job listings.

"""
import os
import faiss
import numpy as np
import json
import google.generativeai as genai
from transformers import AutoTokenizer, AutoModel
import torch
from dotenv import load_dotenv
from datetime import datetime

# Load API key from environment variable (DO NOT hardcode).
load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("Google Gemini API key not found! Set GEMINI_API_KEY.")

genai.configure(api_key=API_KEY)
MODEL_NAME = "gemini-1.5-flash"

# Log file for storing queries and results.
LOG_FILE = "query_logs.json"


def log_interaction(query, retrieved_jobs, ai_response):
    """
    Log user queries, retrieved job results, and AI-generated responses.

    Args:
        query (str): The user's search query.
        retrieved_jobs (list): List of job listings retrieved from FAISS.
        ai_response (str): The response generated by the AI model.
    """
    log_entry = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "query": query,
        "retrieved_jobs": retrieved_jobs,
        "ai_response": ai_response,
    }

    # Load existing logs if the file exists.
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            logs = json.load(f)
    else:
        logs = []

    # Append the new entry.
    logs.append(log_entry)

    # Save back to the log file.
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=4)

    print("Interaction logged!")


def load_faiss_index(index_path="job_index.faiss"):
    """
    Load the FAISS index from the specified file.

    Args:
        index_path (str): Path to the FAISS index file.

    Returns:
        faiss.Index: Loaded FAISS index.

    Raises:
        ValueError: If the FAISS index fails to load.
    """
    try:
        index = faiss.read_index(index_path)
        print("FAISS index loaded successfully!")
        return index
    except Exception as e:
        raise ValueError(f"Error loading FAISS index: {e}")


def load_job_data(json_path="job_data.json"):
    """
    Load job data from a JSON file.

    Args:
        json_path (str): Path to the job data JSON file.

    Returns:
        list: List of job listings.

    Raises:
        ValueError: If the job data fails to load.
    """
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            job_data = json.load(f)
        print(f"Loaded {len(job_data)} job listings from JSON.")
        return job_data
    except Exception as e:
        raise ValueError(f"Error loading job data JSON: {e}")


# Load FAISS and job data.
index = load_faiss_index()
job_data = load_job_data()

# Load Hugging Face embedding model.
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)
model = AutoModel.from_pretrained(EMBEDDING_MODEL)


def get_embedding(text):
    """
    Convert query text into an embedding vector.

    Args:
        text (str): User query text.

    Returns:
        np.ndarray: NumPy array of the embedding.
    """
    inputs = tokenizer(
        text, return_tensors="pt", padding=True, truncation=True, max_length=512
    )
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[:, 0, :].numpy().astype("float32")


def search_jobs(query, k=10):
    """
    Search FAISS for job listings and refine the results using Google Gemini.

    Args:
        query (str): User search query.
        k (int): Number of job results to retrieve (default is 10).

    Returns:
        str: Refined job listings as a formatted response.
    """
    # Generate embedding.
    query_embedding = get_embedding(query).reshape(1, -1)  # Ensure 2D for FAISS.

    # Search FAISS index.
    distances, indices = index.search(query_embedding, k)

    # Retrieve matched jobs, filtering out invalid indices.
    matched_jobs = [job_data[idx] for idx in indices[0] if 0 <= idx < len(job_data)]

    if not matched_jobs:
        response = "⚠️ No jobs found. Kindly change your query and try again."
        log_interaction(query, [], response)  # Log the empty search result.
        return response

    # Format job listings for Gemini.
    job_context = "\n\n".join(
        [
            f"🏢 **Company:** {job['company']}\n"
            f"💼 **Required Skills:** {job['requiredSkills']}\n"
            f"📅 **Experience:** {job['experience']} years\n"
            f"📍 **Location:** {job['location']}\n"
            f"🔗 **More Info:** {job['moreInfo']}"
            for job in matched_jobs
        ]
    )

    prompt = f"""
    You are an AI job assistant. Based on the given user query, refine the 
    provided job listings and show the **5 most relevant** ones.

    **Rules:**
    - Select the most relevant jobs based **only on the provided listings**.
    - Display jobs that match the required experience level.
    - Do not add or modify information—only filter and reformat.
    - The output should follow this structure:

      "**[Company Name]** 
      They are looking for candidates with **[Required Skills]** and require 
      **[Experience] years** of experience.  
      The job is based in **[Location]**. 
      If you're interested, click below to apply: [More Info]"

    **User Query:** {query}

    **Job Listings:**
    {job_context}
    """

    # Send to Google Gemini.
    try:
        response = genai.GenerativeModel(MODEL_NAME).generate_content(prompt)
        refined_results = (
            response.text.strip() if hasattr(response, "text") else "⚠️ No response."
        )
    except Exception as e:
        refined_results = f"Error in Gemini API: {e}"

    # Log the interaction.
    log_interaction(query, matched_jobs, refined_results)

    return refined_results
